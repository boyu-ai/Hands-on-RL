{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6章 Dyna-Q算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 简介\n",
    "\n",
    "在强化学习中，“模型”通常指与智能体交互的环境模型，即对环境的状态转移概率和奖励函数进行建模。根据是否具有环境模型，强化学习算法分为两种：**基于模型的强化学习**（model-based reinforcement learning）和**无模型的强化学习**（model-free reinforcement learning）。无模型的强化学习根据智能体与环境交互采样到的数据直接进行策略提升或者价值估计，第5章讨论的两种时序差分算法，即 Sarsa 和 Q-learning 算法，便是两种无模型的强化学习方法，本书在后续章节中将要介绍的方法也大多是无模型的强化学习算法。在基于模型的强化学习中，模型可以是事先知道的，也可以是根据智能体与环境交互采样到的数据学习得到的，然后用这个模型帮助策略提升或者价值估计。第4章讨论的两种动态规划算法，即策略迭代和价值迭代，则是基于模型的强化学习方法，在这两种算法中环境模型是事先已知的。本章即将介绍的 Dyna-Q 算法也是非常基础的基于模型的强化学习算法，不过它的环境模型是通过采样数据估计得到的。\n",
    "\n",
    "强化学习算法有两个重要的评价指标：一个是算法收敛后的策略在初始状态下的期望回报，另一个是样本复杂度，即算法达到收敛结果需要在真实环境中采样的样本数量。基于模型的强化学习算法由于具有一个环境模型，智能体可以额外和环境模型进行交互，对真实环境中样本的需求量往往就会减少，因此通常会比无模型的强化学习算法具有更低的样本复杂度。但是，环境模型可能并不准确，不能完全代替真实环境，因此基于模型的强化学习算法收敛后其策略的期望回报可能不如无模型的强化学习算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Dyna-Q\n",
    "Dyna-Q 算法是一个经典的基于模型的强化学习算法。如图6-1所示，Dyna-Q 使用一种叫做 Q-planning 的方法来基于模型生成一些模拟数据，然后用模拟数据和真实数据一起改进策略。Q-planning 每次选取一个曾经访问过的状态$s$，采取一个曾经在该状态下执行过的动作$a$，通过模型得到转移后的状态$s'$以及奖励$r$，并根据这个模拟数据$(s,a,r,s')$，用 Q-learning 的更新方式来更新动作价值函数。\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/r0lrrn8v93.jpg?imageView2/0/w/480/h/480)\n",
    "<center>图6-1 基于模型的强化学习方法与无模型的强化学习</center>\n",
    "\n",
    "下面我们来看一下 Dyna-Q 算法的具体流程：\n",
    "\n",
    "- 初始化$Q(s,a)$，初始化模型$M(s,a)$\n",
    "- **for** 序列$e=1\\rightarrow E$ **do**:\n",
    "- $\\quad\\quad$ 得到初始状态$s$\n",
    "- $\\quad\\quad$ **for** $t=1\\rightarrow T$ **do**:\n",
    "- $\\quad\\quad\\quad\\quad$ 用$\\epsilon$-贪婪策略根据$Q$选择当前状态$s$下的动作$a$\n",
    "- $\\quad\\quad\\quad\\quad$ 得到环境反馈的$r, s'$\n",
    "- $\\quad\\quad\\quad\\quad$ $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a')-Q(s,a)]$\n",
    "- $\\quad\\quad\\quad\\quad$ $M(s,a) \\leftarrow r, s'$\n",
    "- $\\quad\\quad\\quad\\quad$ **for** 次数 $n=1\\rightarrow N$ **do**:\n",
    "- $\\quad\\quad\\quad\\quad\\quad\\quad$ 随机选择一个曾经访问过的状态$s_m$ \n",
    "- $\\quad\\quad\\quad\\quad\\quad\\quad$ 采取一个曾经在状态$s_m$下执行过的动作$a_m$\n",
    "- $\\quad\\quad\\quad\\quad\\quad\\quad$ $r_m,s'_m \\leftarrow M(s_m, a_m)$\n",
    "- $\\quad\\quad\\quad\\quad\\quad\\quad$ $Q(s_m,a_m) \\leftarrow Q(s_m,a_m) + \\alpha[r_m + \\gamma \\max_{a'} Q(s'_m,a')-Q(s_m,a_m)]$\n",
    "- $\\quad\\quad\\quad\\quad$ **end for**\n",
    "- $\\quad\\quad\\quad\\quad$ $s\\leftarrow s'$\n",
    "- $\\quad\\quad$ **end for**\n",
    "- **end for**\n",
    "\n",
    "\n",
    "可以看到，在每次与环境进行交互执行一次 Q-learning 之后，Dyna-Q 会做$n$次 Q-planning。其中 Q-planning 的次数$N$是一个事先可以选择的超参数，当其为0时就是普通的 Q-learning。值得注意的是，上述 Dyna-Q 算法是执行在一个离散并且确定的环境中，所以当看到一条经验数据$(s,a,r,s')$时，可以直接对模型做出更新，即$M(s,a) \\leftarrow r, s'$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Dyna-Q 代码实践\n",
    "我们在悬崖漫步环境中执行过 Q-learning 算法，现在也在这个环境中实现 Dyna-Q，以方便比较。首先仍需要实现悬崖漫步的环境代码，和5.3节一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "class CliffWalkingEnv:\n",
    "    def __init__(self, ncol, nrow):\n",
    "        self.nrow = nrow\n",
    "        self.ncol = ncol\n",
    "        self.x = 0  # 记录当前智能体位置的横坐标\n",
    "        self.y = self.nrow - 1  # 记录当前智能体位置的纵坐标\n",
    "\n",
    "    def step(self, action):  # 外部调用这个函数来改变当前位置\n",
    "        # 4种动作, change[0]:上, change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n",
    "        # 定义在左上角\n",
    "        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        self.x = min(self.ncol - 1, max(0, self.x + change[action][0]))\n",
    "        self.y = min(self.nrow - 1, max(0, self.y + change[action][1]))\n",
    "        next_state = self.y * self.ncol + self.x\n",
    "        reward = -1\n",
    "        done = False\n",
    "        if self.y == self.nrow - 1 and self.x > 0:  # 下一个位置在悬崖或者目标\n",
    "            done = True\n",
    "            if self.x != self.ncol - 1:\n",
    "                reward = -100\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def reset(self):  # 回归初始状态,起点在左上角\n",
    "        self.x = 0\n",
    "        self.y = self.nrow - 1\n",
    "        return self.y * self.ncol + self.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们在 Q-learning 的代码上进行简单修改，实现 Dyna-Q 的主要代码。最主要的修改是加入了环境模型`model`，用一个字典表示，每次在真实环境中收集到新的数据，就把它加入字典。根据字典的性质，若该数据本身存在于字典中，便不会再一次进行添加。在 Dyna-Q 的更新中，执行完 Q-learning 后，会立即执行 Q-planning。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQ:\n",
    "    \"\"\" Dyna-Q算法 \"\"\"\n",
    "    def __init__(self,\n",
    "                 ncol,\n",
    "                 nrow,\n",
    "                 epsilon,\n",
    "                 alpha,\n",
    "                 gamma,\n",
    "                 n_planning,\n",
    "                 n_action=4):\n",
    "        self.Q_table = np.zeros([nrow * ncol, n_action])  # 初始化Q(s,a)表格\n",
    "        self.n_action = n_action  # 动作个数\n",
    "        self.alpha = alpha  # 学习率\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.epsilon = epsilon  # epsilon-贪婪策略中的参数\n",
    "\n",
    "        self.n_planning = n_planning  #执行Q-planning的次数, 对应1次Q-learning\n",
    "        self.model = dict()  # 环境模型\n",
    "\n",
    "    def take_action(self, state):  # 选取下一步的操作\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.n_action)\n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[state])\n",
    "        return action\n",
    "\n",
    "    def q_learning(self, s0, a0, r, s1):\n",
    "        td_error = r + self.gamma * self.Q_table[s1].max(\n",
    "        ) - self.Q_table[s0, a0]\n",
    "        self.Q_table[s0, a0] += self.alpha * td_error\n",
    "\n",
    "    def update(self, s0, a0, r, s1):\n",
    "        self.q_learning(s0, a0, r, s1)\n",
    "        self.model[(s0, a0)] = r, s1  # 将数据添加到模型中\n",
    "        for _ in range(self.n_planning):  # Q-planning循环\n",
    "            # 随机选择曾经遇到过的状态动作对\n",
    "            (s, a), (r, s_) = random.choice(list(self.model.items()))\n",
    "            self.q_learning(s, a, r, s_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是 Dyna-Q 算法在悬崖漫步环境中的训练函数，它的输入参数是 Q-planning 的步数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DynaQ_CliffWalking(n_planning):\n",
    "    ncol = 12\n",
    "    nrow = 4\n",
    "    env = CliffWalkingEnv(ncol, nrow)\n",
    "    epsilon = 0.01\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    agent = DynaQ(ncol, nrow, epsilon, alpha, gamma, n_planning)\n",
    "    num_episodes = 300  # 智能体在环境中运行多少条序列\n",
    "\n",
    "    return_list = []  # 记录每一条序列的回报\n",
    "    for i in range(10):  # 显示10个进度条\n",
    "        # tqdm的进度条功能\n",
    "        with tqdm(total=int(num_episodes / 10),\n",
    "                  desc='Iteration %d' % i) as pbar:\n",
    "            for i_episode in range(int(num_episodes / 10)):  # 每个进度条的序列数\n",
    "                episode_return = 0\n",
    "                state = env.reset()\n",
    "                done = False\n",
    "                while not done:\n",
    "                    action = agent.take_action(state)\n",
    "                    next_state, reward, done = env.step(action)\n",
    "                    episode_return += reward  # 这里回报的计算不进行折扣因子衰减\n",
    "                    agent.update(state, action, reward, next_state)\n",
    "                    state = next_state\n",
    "                return_list.append(episode_return)\n",
    "                if (i_episode + 1) % 10 == 0:  # 每10条序列打印一下这10条序列的平均回报\n",
    "                    pbar.set_postfix({\n",
    "                        'episode':\n",
    "                        '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                        'return':\n",
    "                        '%.3f' % np.mean(return_list[-10:])\n",
    "                    })\n",
    "                pbar.update(1)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来对结果进行可视化，通过调整参数，我们可以观察 Q-planning 步数对结果的影响（另见彩插图3）。若 Q-planning 步数为0，Dyna-Q 算法则退化为 Q-learning。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "n_planning_list = [0, 2, 20]\n",
    "for n_planning in n_planning_list:\n",
    "    print('Q-planning步数为：%d' % n_planning)\n",
    "    time.sleep(0.5)\n",
    "    return_list = DynaQ_CliffWalking(n_planning)\n",
    "    episodes_list = list(range(len(return_list)))\n",
    "    plt.plot(episodes_list,\n",
    "             return_list,\n",
    "             label=str(n_planning) + ' planning steps')\n",
    "plt.legend()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('Dyna-Q on {}'.format('Cliff Walking'))\n",
    "plt.show()\n",
    "\n",
    "# Q-planning步数为：0\n",
    "\n",
    "# Iteration 0: 100%|██████████| 30/30 [00:00<00:00, 615.42it/s, episode=30,\n",
    "# return=-138.400]\n",
    "# Iteration 1: 100%|██████████| 30/30 [00:00<00:00, 1079.50it/s, episode=60,\n",
    "# return=-64.100]\n",
    "# Iteration 2: 100%|██████████| 30/30 [00:00<00:00, 1303.35it/s, episode=90,\n",
    "# return=-46.000]\n",
    "# Iteration 3: 100%|██████████| 30/30 [00:00<00:00, 1169.51it/s, episode=120,\n",
    "# return=-38.000]\n",
    "# Iteration 4: 100%|██████████| 30/30 [00:00<00:00, 1806.96it/s, episode=150,\n",
    "# return=-28.600]\n",
    "# Iteration 5: 100%|██████████| 30/30 [00:00<00:00, 2303.21it/s, episode=180,\n",
    "# return=-25.300]\n",
    "# Iteration 6: 100%|██████████| 30/30 [00:00<00:00, 2473.64it/s, episode=210,\n",
    "# return=-23.600]\n",
    "# Iteration 7: 100%|██████████| 30/30 [00:00<00:00, 2344.37it/s, episode=240,\n",
    "# return=-20.100]\n",
    "# Iteration 8: 100%|██████████| 30/30 [00:00<00:00, 1735.84it/s, episode=270,\n",
    "# return=-17.100]\n",
    "# Iteration 9: 100%|██████████| 30/30 [00:00<00:00, 2827.94it/s, episode=300,\n",
    "# return=-16.500]\n",
    "\n",
    "# Q-planning步数为：2\n",
    "\n",
    "# Iteration 0: 100%|██████████| 30/30 [00:00<00:00, 425.09it/s, episode=30,\n",
    "# return=-53.800]\n",
    "# Iteration 1: 100%|██████████| 30/30 [00:00<00:00, 655.71it/s, episode=60,\n",
    "# return=-37.100]\n",
    "# Iteration 2: 100%|██████████| 30/30 [00:00<00:00, 799.69it/s, episode=90,\n",
    "# return=-23.600]\n",
    "# Iteration 3: 100%|██████████| 30/30 [00:00<00:00, 915.34it/s, episode=120,\n",
    "# return=-18.500]\n",
    "# Iteration 4: 100%|██████████| 30/30 [00:00<00:00, 1120.39it/s, episode=150,\n",
    "# return=-16.400]\n",
    "# Iteration 5: 100%|██████████| 30/30 [00:00<00:00, 1437.24it/s, episode=180,\n",
    "# return=-16.400]\n",
    "# Iteration 6: 100%|██████████| 30/30 [00:00<00:00, 1366.79it/s, episode=210,\n",
    "# return=-13.400]\n",
    "# Iteration 7: 100%|██████████| 30/30 [00:00<00:00, 1457.62it/s, episode=240,\n",
    "# return=-13.200]\n",
    "# Iteration 8: 100%|██████████| 30/30 [00:00<00:00, 1743.68it/s, episode=270,\n",
    "# return=-13.200]\n",
    "# Iteration 9: 100%|██████████| 30/30 [00:00<00:00, 1699.59it/s, episode=300,\n",
    "# return=-13.500]\n",
    "\n",
    "# Q-planning步数为：20\n",
    "\n",
    "# Iteration 0: 100%|██████████| 30/30 [00:00<00:00, 143.91it/s, episode=30,\n",
    "# return=-18.500]\n",
    "# Iteration 1: 100%|██████████| 30/30 [00:00<00:00, 268.53it/s, episode=60,\n",
    "# return=-13.600]\n",
    "# Iteration 2: 100%|██████████| 30/30 [00:00<00:00, 274.53it/s, episode=90,\n",
    "# return=-13.000]\n",
    "# Iteration 3: 100%|██████████| 30/30 [00:00<00:00, 264.25it/s, episode=120,\n",
    "# return=-13.500]\n",
    "# Iteration 4: 100%|██████████| 30/30 [00:00<00:00, 263.58it/s, episode=150,\n",
    "# return=-13.500]\n",
    "# Iteration 5: 100%|██████████| 30/30 [00:00<00:00, 245.27it/s, episode=180,\n",
    "# return=-13.000]\n",
    "# Iteration 6: 100%|██████████| 30/30 [00:00<00:00, 257.16it/s, episode=210,\n",
    "# return=-22.000]\n",
    "# Iteration 7: 100%|██████████| 30/30 [00:00<00:00, 257.08it/s, episode=240,\n",
    "# return=-23.200]\n",
    "# Iteration 8: 100%|██████████| 30/30 [00:00<00:00, 261.12it/s, episode=270,\n",
    "# return=-13.000]\n",
    "# Iteration 9: 100%|██████████| 30/30 [00:00<00:00, 213.01it/s, episode=300,\n",
    "# return=-13.400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述结果中我们可以很容易地看出，随着 Q-planning 步数的增多，Dyna-Q 算法的收敛速度也随之变快。当然，并不是在所有的环境中，都是 Q-planning 步数越大则算法收敛越快，这取决于环境是否是确定性的，以及环境模型的精度。在上述悬崖漫步环境中，状态的转移是完全确定性的，构建的环境模型的精度是最高的，所以可以通过增加 Q-planning 步数来直接降低算法的样本复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 小结\n",
    "本章讲解了一个经典的基于模型的强化学习算法 Dyna-Q，并且通过调整在悬崖漫步环境下的 Q-planning 步数，直观地展示了 Q-planning 步数对于收敛速度的影响。我们发现基于模型的强化学习算法 Dyna-Q 在以上环境中获得了很好的效果，但这些环境比较简单，模型可以直接通过经验数据得到。如果环境比较复杂，状态是连续的，或者状态转移是随机的而不是决定性的，如何学习一个比较准确的模型就变成非常重大的挑战，这直接影响到基于模型的强化学习算法能否应用于这些环境并获得比无模型的强化学习更好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5  参考文献\n",
    "[1]\tSUTTON R S. Dyna, an integrated architecture for learning, planning, and reacting [J]. ACM Sigart Bulletin, 1991 2(4): 160-163.\n",
    "\n",
    "[2]\tSUTTON, R S. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming [C]// Proc of International Conference on Machine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
