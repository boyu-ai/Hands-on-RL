{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import rl_utils\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "! git clone https://github.com/boyu-ai/ma-gym.git\n",
    "import sys\n",
    "sys.path.append(\"./ma-gym\")\n",
    "from ma_gym.envs.combat.combat import Combat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "    super(PolicyNet, self).__init__()\n",
    "    self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "    self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.fc3 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "def forward(self, x):\n",
    "    x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "    return  F.softmax(self.fc3(x),dim=1)\n",
    "\n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "    super(ValueNet, self).__init__()\n",
    "    self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "    self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.fc3 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class PPO:\n",
    "    ''' PPO算法,采用截断方式 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, \n",
    "            eps, gamma, device):\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.eps = eps # PPO中截断范围的参数\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.\n",
    "                device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).\n",
    "                view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).\n",
    "                to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).\n",
    "                to(self.device)\n",
    "        td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)\n",
    "        td_delta = td_target - self.critic(states)\n",
    "        advantage = rl_utils.compute_advantage(self.gamma, self.lmbda, td_delta.\n",
    "                cpu()).to(self.device)\n",
    "        old_log_probs = torch.log(self.actor(states).gather(1, actions)).detach()\n",
    "\n",
    "        log_probs = torch.log(self.actor(states).gather(1, actions))\n",
    "        ratio = torch.exp(log_probs - old_log_probs)\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1-self.eps, 1+self.eps) * advantage # 截断\n",
    "        actor_loss = torch.mean(-torch.min(surr1, surr2)) # PPO损失函数\n",
    "        critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_lr = 3e-4\n",
    "critic_lr = 1e-3\n",
    "num_episodes = 100000\n",
    "hidden_dim = 64\n",
    "gamma = 0.99\n",
    "lmbda = 0.97\n",
    "eps = 0.2\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "team_size = 2 \n",
    "grid_size = (15, 15) \n",
    "#创建Combat环境，格子世界的大小为15x15，己方智能体和敌方智能体数量都为2\n",
    "env = Combat(grid_shape=grid_size, n_agents=team_size, n_opponents=team_size)\n",
    "\n",
    "state_dim = env.observation_space[0].shape[0]\n",
    "action_dim = env.action_space[0].n\n",
    "#两个智能体共享同一个策略\n",
    "agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, eps, gamma, device)\n",
    "\n",
    "win_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_episodes/10), desc='Iteration %d' % i) as pbar:\n",
    "    for i_episode in range(int(num_episodes/10)):\n",
    "        transition_dict_1 = {'states': [], 'actions': [], 'next_states': [], \n",
    "                'rewards': [], 'dones': []}\n",
    "        transition_dict_2 = {'states': [], 'actions': [], 'next_states': [], \n",
    "                'rewards': [], 'dones': []}\n",
    "        s = env.reset()\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            a_1 = agent.take_action(s[0])\n",
    "            a_2 = agent.take_action(s[1])\n",
    "            next_s, r, done, info = env.step([a_1, a_2])\n",
    "            transition_dict_1['states'].append(s[0])\n",
    "            transition_dict_1['actions'].append(a_1)\n",
    "            transition_dict_1['next_states'].append(next_s[0])\n",
    "            transition_dict_1['rewards'].append(r[0]+100 if info['win'] else r[0]-0.1)\n",
    "            transition_dict_1['dones'].append(False)\n",
    "            transition_dict_2['states'].append(s[1])\n",
    "            transition_dict_2['actions'].append(a_2)\n",
    "            transition_dict_2['next_states'].append(next_s[1])\n",
    "            transition_dict_2['rewards'].append(r[1]+100 if info['win'] else r[1]-0.1)\n",
    "            transition_dict_2['dones'].append(False)\n",
    "            s = next_s\n",
    "            terminal = all(done)\n",
    "        win_list.append(1 if info[\"win\"] else 0)\n",
    "        agent.update(transition_dict_1)\n",
    "        agent.update(transition_dict_2)\n",
    "        if (i_episode+1) % 100 == 0:\n",
    "            pbar.set_postfix({'episode': '%d' % (num_episodes/10 * i + i_episode+1), \n",
    "                    'return': '%.3f' % np.mean(win_list[-100:])})\n",
    "        pbar.update(1)\n",
    "\n",
    "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning:[33mWARN: \n",
    "Box bound precision lowered by casting to float32[0m\n",
    "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
    "\n",
    "# Iteration 0: 100%|██████████| 10000/10000 [05:22<00:00, 31.02it/s, episode=10000,\n",
    "# return=0.220]\n",
    "# Iteration 1: 100%|██████████| 10000/10000 [04:03<00:00, 41.07it/s, episode=20000,\n",
    "# return=0.400]\n",
    "# Iteration 2: 100%|██████████| 10000/10000 [03:37<00:00, 45.96it/s, episode=30000,\n",
    "# return=0.670]\n",
    "# Iteration 3: 100%|██████████| 10000/10000 [03:13<00:00, 51.55it/s, episode=40000,\n",
    "# return=0.590]\n",
    "# Iteration 4: 100%|██████████| 10000/10000 [02:58<00:00, 56.07it/s, episode=50000,\n",
    "# return=0.750]\n",
    "# Iteration 5: 100%|██████████| 10000/10000 [02:58<00:00, 56.09it/s, episode=60000,\n",
    "# return=0.660]\n",
    "# Iteration 6: 100%|██████████| 10000/10000 [02:57<00:00, 56.42it/s, episode=70000,\n",
    "# return=0.660]\n",
    "# Iteration 7: 100%|██████████| 10000/10000 [03:04<00:00, 54.20it/s, episode=80000,\n",
    "# return=0.720]\n",
    "# Iteration 8: 100%|██████████| 10000/10000 [02:59<00:00, 55.84it/s, episode=90000,\n",
    "# return=0.530]\n",
    "# Iteration 9: 100%|██████████| 10000/10000 [03:03<00:00, 54.55it/s, episode=100000,\n",
    "# return=0.710]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_array = np.array(win_list)\n",
    "#每100条轨迹取一次平均\n",
    "win_array = np.mean(win_array.reshape(-1, 100), axis=1)\n",
    "\n",
    "episodes_list = np.arange(win_array.shape[0]) * 100\n",
    "plt.plot(episodes_list, win_array)\n",
    "plt.xlabel('Episodes') \n",
    "plt.ylabel('Win rate') \n",
    "plt.title('IPPO on Combat') \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
