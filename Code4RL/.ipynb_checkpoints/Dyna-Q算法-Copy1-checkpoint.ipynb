{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "class CliffWalkingEnv:\n",
    "    def __init__(self, ncol, nrow):\n",
    "        self.nrow = nrow       \n",
    "        self.ncol = ncol\n",
    "        self.x = 0 # 记录当前智能体位置的横坐标\n",
    "        self.y = self.nrow - 1 # 记录当前智能体位置的纵坐标\n",
    "\n",
    "    def step(self, action): # 外部调用这个函数来改变当前位置\n",
    "        # 4种动作, change[0]:上, change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n",
    "        # 定义在左上角\n",
    "        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        self.x = min(self.ncol - 1, max(0, self.x + change[action][0]))\n",
    "        self.y = min(self.nrow - 1, max(0, self.y + change[action][1]))\n",
    "        next_state = self.y * self.ncol + self.x\n",
    "        reward = -1\n",
    "        done = False\n",
    "        if self.y == self.nrow - 1 and self.x > 0: # 下一个位置在悬崖或者目标\n",
    "            done = True\n",
    "            if self.x != self.ncol - 1:\n",
    "                reward = -100\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def reset(self): # 回归初始状态,起点在左上角\n",
    "        self.x = 0\n",
    "        self.y = self.nrow - 1\n",
    "        return self.y * self.ncol + self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQ:\n",
    "    \"\"\" Dyna-Q算法 \"\"\"\n",
    "    def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_planning, n_action=4):\n",
    "        self.Q_table = np.zeros([nrow * ncol, n_action]) # 初始化Q(s,a)表格\n",
    "        self.n_action = n_action # 动作个数\n",
    "        self.alpha = alpha # 学习率\n",
    "        self.gamma = gamma # 折扣因子\n",
    "        self.epsilon = epsilon # epsilon-贪婪策略中的参数\n",
    "        \n",
    "        self.n_planning = n_planning #执行Q-planning的次数, 对应1次Q-learning\n",
    "        self.model = dict() # 环境模型\n",
    "        \n",
    "    def take_action(self, state): # 选取下一步的操作\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.n_action)\n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[state])\n",
    "        return action\n",
    "\n",
    "    def q_learning(self, s0, a0, r, s1):\n",
    "        td_error = r + self.gamma * self.Q_table[s1].max() - self.Q_table[s0, a0]\n",
    "        self.Q_table[s0, a0] += self.alpha * td_error\n",
    "            \n",
    "    def update(self, s0, a0, r, s1):\n",
    "        self.q_learning(s0, a0, r, s1)\n",
    "        self.model[(s0, a0)] = r, s1 # 将数据添加到模型中\n",
    "        for _ in range(self.n_planning): # Q-planning循环\n",
    "            # 随机选择曾经遇到过的状态动作对\n",
    "            (s, a), (r, s_) = random.choice(list(self.model.items()))\n",
    "            self.q_learning(s, a, r, s_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DynaQ_CliffWalking(n_planning):\n",
    "    ncol = 12\n",
    "    nrow = 4\n",
    "    env = CliffWalkingEnv(ncol, nrow)\n",
    "    epsilon = 0.01\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    agent = DynaQ(ncol, nrow, epsilon, alpha, gamma, n_planning)\n",
    "    num_episodes = 300 # 智能体在环境中运行多少条序列\n",
    "\n",
    "    return_list = [] # 记录每一条序列的回报\n",
    "    for i in range(10): # 显示10个进度条\n",
    "        # tqdm的进度条功能\n",
    "        with tqdm(total=int(num_episodes/10), desc='Iteration %d' % i) as pbar: \n",
    "            for i_episode in range(int(num_episodes/10)): # 每个进度条的序列数\n",
    "                episode_return = 0\n",
    "                state = env.reset()\n",
    "                done = False\n",
    "                while not done: \n",
    "                    action = agent.take_action(state)\n",
    "                    next_state, reward, done = env.step(action)\n",
    "                    episode_return += reward # 这里回报的计算不进行折扣因子衰减\n",
    "                    agent.update(state, action, reward, next_state)\n",
    "                    state = next_state\n",
    "                return_list.append(episode_return) \n",
    "                if (i_episode+1) % 10 == 0: # 每10条序列打印一下这10条序列的平均回报\n",
    "                    pbar.set_postfix({'episode': '%d' % (num_episodes / 10 * \n",
    "                            i + i_episode+1), 'return': '%.3f' % np.mean\n",
    "                            (return_list[-10:])})\n",
    "                pbar.update(1)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "n_planning_list = [0, 2, 20]\n",
    "for n_planning in n_planning_list:\n",
    "    print('Q-planning步数为：%d' % n_planning)\n",
    "    time.sleep(0.5)\n",
    "    return_list = DynaQ_CliffWalking(n_planning)\n",
    "    episodes_list = list(range(len(return_list)))\n",
    "    plt.plot(episodes_list, return_list, label=str(n_planning) + ' planning steps')\n",
    "plt.legend()\n",
    "plt.xlabel('Episodes')   \n",
    "plt.ylabel('Returns')  \n",
    "plt.title('Dyna-Q on {}'.format('Cliff Walking'))  \n",
    "plt.show()\n",
    "\n",
    "# Q-planning步数为：0\n",
    "\n",
    "# Iteration 0: 100%|██████████| 30/30 [00:00<00:00, 615.42it/s, episode=30, \n",
    "# return=-138.400]\n",
    "# Iteration 1: 100%|██████████| 30/30 [00:00<00:00, 1079.50it/s, episode=60, \n",
    "# return=-64.100]\n",
    "# Iteration 2: 100%|██████████| 30/30 [00:00<00:00, 1303.35it/s, episode=90, \n",
    "# return=-46.000]\n",
    "# Iteration 3: 100%|██████████| 30/30 [00:00<00:00, 1169.51it/s, episode=120, \n",
    "# return=-38.000]\n",
    "# Iteration 4: 100%|██████████| 30/30 [00:00<00:00, 1806.96it/s, episode=150, \n",
    "# return=-28.600]\n",
    "# Iteration 5: 100%|██████████| 30/30 [00:00<00:00, 2303.21it/s, episode=180, \n",
    "# return=-25.300]\n",
    "# Iteration 6: 100%|██████████| 30/30 [00:00<00:00, 2473.64it/s, episode=210, \n",
    "# return=-23.600]\n",
    "# Iteration 7: 100%|██████████| 30/30 [00:00<00:00, 2344.37it/s, episode=240, \n",
    "# return=-20.100]\n",
    "# Iteration 8: 100%|██████████| 30/30 [00:00<00:00, 1735.84it/s, episode=270, \n",
    "# return=-17.100]\n",
    "# Iteration 9: 100%|██████████| 30/30 [00:00<00:00, 2827.94it/s, episode=300, \n",
    "# return=-16.500]\n",
    "\n",
    "# Q-planning步数为：2\n",
    "\n",
    "# Iteration 0: 100%|██████████| 30/30 [00:00<00:00, 425.09it/s, episode=30, \n",
    "# return=-53.800]\n",
    "# Iteration 1: 100%|██████████| 30/30 [00:00<00:00, 655.71it/s, episode=60, \n",
    "# return=-37.100]\n",
    "# Iteration 2: 100%|██████████| 30/30 [00:00<00:00, 799.69it/s, episode=90, \n",
    "# return=-23.600]\n",
    "# Iteration 3: 100%|██████████| 30/30 [00:00<00:00, 915.34it/s, episode=120, \n",
    "# return=-18.500]\n",
    "# Iteration 4: 100%|██████████| 30/30 [00:00<00:00, 1120.39it/s, episode=150, \n",
    "# return=-16.400]\n",
    "# Iteration 5: 100%|██████████| 30/30 [00:00<00:00, 1437.24it/s, episode=180, \n",
    "# return=-16.400]\n",
    "# Iteration 6: 100%|██████████| 30/30 [00:00<00:00, 1366.79it/s, episode=210, \n",
    "# return=-13.400]\n",
    "# Iteration 7: 100%|██████████| 30/30 [00:00<00:00, 1457.62it/s, episode=240, \n",
    "# return=-13.200]\n",
    "# Iteration 8: 100%|██████████| 30/30 [00:00<00:00, 1743.68it/s, episode=270, \n",
    "# return=-13.200]\n",
    "# Iteration 9: 100%|██████████| 30/30 [00:00<00:00, 1699.59it/s, episode=300, \n",
    "# return=-13.500]\n",
    "\n",
    "# Q-planning步数为：20\n",
    "\n",
    "# Iteration 0: 100%|██████████| 30/30 [00:00<00:00, 143.91it/s, episode=30, \n",
    "# return=-18.500]\n",
    "# Iteration 1: 100%|██████████| 30/30 [00:00<00:00, 268.53it/s, episode=60, \n",
    "# return=-13.600]\n",
    "# Iteration 2: 100%|██████████| 30/30 [00:00<00:00, 274.53it/s, episode=90, \n",
    "# return=-13.000]\n",
    "# Iteration 3: 100%|██████████| 30/30 [00:00<00:00, 264.25it/s, episode=120, \n",
    "# return=-13.500]\n",
    "# Iteration 4: 100%|██████████| 30/30 [00:00<00:00, 263.58it/s, episode=150, \n",
    "# return=-13.500]\n",
    "# Iteration 5: 100%|██████████| 30/30 [00:00<00:00, 245.27it/s, episode=180, \n",
    "# return=-13.000]\n",
    "# Iteration 6: 100%|██████████| 30/30 [00:00<00:00, 257.16it/s, episode=210, \n",
    "# return=-22.000]\n",
    "# Iteration 7: 100%|██████████| 30/30 [00:00<00:00, 257.08it/s, episode=240, \n",
    "# return=-23.200]\n",
    "# Iteration 8: 100%|██████████| 30/30 [00:00<00:00, 261.12it/s, episode=270, \n",
    "# return=-13.000]\n",
    "# Iteration 9: 100%|██████████| 30/30 [00:00<00:00, 213.01it/s, episode=300, \n",
    "# return=-13.400]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
